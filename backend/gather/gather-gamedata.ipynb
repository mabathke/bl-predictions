{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Historic Data\n",
    "This notebook is, for now, more of experimental work. I want to gather multiple sources of data - preferably automate this process. For starters I want to gather historical data since they are more of a one time job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Start with all results from the first BL\n",
    "\n",
    "I found multiple websites to gather. I took this one that is the simplest to scrap. Since no `robot.txt` is present I dont think this is a problem, because I only want to gather this one time only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: http://www.bulibox.de/spieltage/B100101.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100102.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100103.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100104.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100105.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100106.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100107.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100108.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100109.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100110.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100111.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100112.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100113.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100114.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100115.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100116.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100117.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100118.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100119.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100120.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100121.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100122.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100123.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100124.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100125.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100126.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100127.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100128.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100129.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100130.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100131.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100132.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100133.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100134.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100135.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100136.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100137.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100138.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100139.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100140.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100141.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100142.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100143.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100144.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100145.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100146.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100147.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100148.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100149.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100150.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100151.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100152.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100153.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100154.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100155.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100156.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100157.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100158.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100159.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100160.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100161.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# List to store scraped data\n",
    "data = []\n",
    "\n",
    "# Iterate over the pages (from 101 to 161 inclusive)\n",
    "for i in range(101, 162):\n",
    "    url = f\"http://www.bulibox.de/spieltage/B100{i}.html\"\n",
    "    print(f\"Scraping: {url}\")\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the page was retrieved successfully\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error retrieving {url}\")\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Get season info from the <h4> tag (if available)\n",
    "    h4_tag = soup.find(\"h4\")\n",
    "    season = h4_tag.text.strip() if h4_tag else \"Unknown Season\"\n",
    "    \n",
    "    # Find all matchday markers (<b class=\"bulired\">)\n",
    "    matchday_tags = soup.find_all(\"b\", class_=\"bulired\")\n",
    "    \n",
    "    for matchday_tag in matchday_tags:\n",
    "        matchday = matchday_tag.text.strip()\n",
    "        # Get the table that immediately follows this matchday header\n",
    "        table = matchday_tag.find_next(\"table\")\n",
    "        if table:\n",
    "            rows = table.find_all(\"tr\")[1:]  # skip header row\n",
    "            for row in rows:\n",
    "                cells = row.find_all(\"td\")\n",
    "                if len(cells) >= 3:\n",
    "                    spielpaarung = cells[0].get_text(strip=True)\n",
    "                    ergebnis = cells[1].get_text(strip=True)\n",
    "                    datum = cells[2].get_text(strip=True)\n",
    "                    \n",
    "                    data.append({\n",
    "                        \"Season\": season,\n",
    "                        \"Spieltag\": matchday,\n",
    "                        \"Spielpaarung\": spielpaarung,\n",
    "                        \"Ergebnis\": ergebnis,\n",
    "                        \"Datum\": datum\n",
    "                    })\n",
    "    \n",
    "    # Wait 5 seconds before the next request to be compliant with the site's usage\n",
    "    time.sleep(5)\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/first/first_bl_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather all tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100101.html\n",
      "Saved table for season 1963-1964 to data/abschlusstabellen\\1963-1964.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100102.html\n",
      "Saved table for season 1964-1965 to data/abschlusstabellen\\1964-1965.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100103.html\n",
      "Saved table for season 1965-1966 to data/abschlusstabellen\\1965-1966.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100104.html\n",
      "Saved table for season 1966-1967 to data/abschlusstabellen\\1966-1967.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100105.html\n",
      "Saved table for season 1967-1968 to data/abschlusstabellen\\1967-1968.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100106.html\n",
      "Saved table for season 1968-1969 to data/abschlusstabellen\\1968-1969.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100107.html\n",
      "Saved table for season 1969-1970 to data/abschlusstabellen\\1969-1970.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100108.html\n",
      "Saved table for season 1970-1971 to data/abschlusstabellen\\1970-1971.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100109.html\n",
      "Saved table for season 1971-1972 to data/abschlusstabellen\\1971-1972.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100110.html\n",
      "Saved table for season 1972-1973 to data/abschlusstabellen\\1972-1973.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100111.html\n",
      "Saved table for season 1973-1974 to data/abschlusstabellen\\1973-1974.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100112.html\n",
      "Saved table for season 1974-1975 to data/abschlusstabellen\\1974-1975.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100113.html\n",
      "Saved table for season 1975-1976 to data/abschlusstabellen\\1975-1976.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100114.html\n",
      "Saved table for season 1976-1977 to data/abschlusstabellen\\1976-1977.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100115.html\n",
      "Saved table for season 1977-1978 to data/abschlusstabellen\\1977-1978.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100116.html\n",
      "Saved table for season 1978-1979 to data/abschlusstabellen\\1978-1979.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100117.html\n",
      "Saved table for season 1979-1980 to data/abschlusstabellen\\1979-1980.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100118.html\n",
      "Saved table for season 1980-1981 to data/abschlusstabellen\\1980-1981.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100119.html\n",
      "Saved table for season 1981-1982 to data/abschlusstabellen\\1981-1982.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100120.html\n",
      "Saved table for season 1982-1983 to data/abschlusstabellen\\1982-1983.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100121.html\n",
      "Saved table for season 1983-1984 to data/abschlusstabellen\\1983-1984.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100122.html\n",
      "Saved table for season 1984-1985 to data/abschlusstabellen\\1984-1985.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100123.html\n",
      "Saved table for season 1985-1986 to data/abschlusstabellen\\1985-1986.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100124.html\n",
      "Saved table for season 1986-1987 to data/abschlusstabellen\\1986-1987.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100125.html\n",
      "Saved table for season 1987-1988 to data/abschlusstabellen\\1987-1988.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100126.html\n",
      "Saved table for season 1988-1989 to data/abschlusstabellen\\1988-1989.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100127.html\n",
      "Saved table for season 1989-1990 to data/abschlusstabellen\\1989-1990.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100128.html\n",
      "Saved table for season 1990-1991 to data/abschlusstabellen\\1990-1991.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100129.html\n",
      "Saved table for season 1991-1992 to data/abschlusstabellen\\1991-1992.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100130.html\n",
      "Saved table for season 1992-1993 to data/abschlusstabellen\\1992-1993.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100131.html\n",
      "Saved table for season 1993-1994 to data/abschlusstabellen\\1993-1994.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100132.html\n",
      "Saved table for season 1994-1995 to data/abschlusstabellen\\1994-1995.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100133.html\n",
      "Saved table for season 1995-1996 to data/abschlusstabellen\\1995-1996.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100134.html\n",
      "Saved table for season 1996-1997 to data/abschlusstabellen\\1996-1997.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100135.html\n",
      "Saved table for season 1997-1998 to data/abschlusstabellen\\1997-1998.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100136.html\n",
      "Saved table for season 1998-1999 to data/abschlusstabellen\\1998-1999.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100137.html\n",
      "Saved table for season 1999-2000 to data/abschlusstabellen\\1999-2000.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100138.html\n",
      "Saved table for season 2000-2001 to data/abschlusstabellen\\2000-2001.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100139.html\n",
      "Saved table for season 2001-2002 to data/abschlusstabellen\\2001-2002.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100140.html\n",
      "Saved table for season 2002-2003 to data/abschlusstabellen\\2002-2003.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100141.html\n",
      "Saved table for season 2003-2004 to data/abschlusstabellen\\2003-2004.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100142.html\n",
      "Saved table for season 2004-2005 to data/abschlusstabellen\\2004-2005.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100143.html\n",
      "Saved table for season 2005-2006 to data/abschlusstabellen\\2005-2006.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100144.html\n",
      "Saved table for season 2006-2007 to data/abschlusstabellen\\2006-2007.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100145.html\n",
      "Saved table for season 2007-2008 to data/abschlusstabellen\\2007-2008.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100146.html\n",
      "Saved table for season 2008-2009 to data/abschlusstabellen\\2008-2009.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100147.html\n",
      "Saved table for season 2009-2010 to data/abschlusstabellen\\2009-2010.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100148.html\n",
      "Saved table for season 2010-2011 to data/abschlusstabellen\\2010-2011.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100149.html\n",
      "Saved table for season 2011-2012 to data/abschlusstabellen\\2011-2012.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100150.html\n",
      "Saved table for season 2012-2013 to data/abschlusstabellen\\2012-2013.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100151.html\n",
      "Saved table for season 2013-2014 to data/abschlusstabellen\\2013-2014.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100152.html\n",
      "Saved table for season 2014-2015 to data/abschlusstabellen\\2014-2015.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100153.html\n",
      "Saved table for season 2015-2016 to data/abschlusstabellen\\2015-2016.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100154.html\n",
      "Saved table for season 2016-2017 to data/abschlusstabellen\\2016-2017.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100155.html\n",
      "Saved table for season 2017-2018 to data/abschlusstabellen\\2017-2018.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100156.html\n",
      "Saved table for season 2018-2019 to data/abschlusstabellen\\2018-2019.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100157.html\n",
      "Saved table for season 2019-2020 to data/abschlusstabellen\\2019-2020.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100158.html\n",
      "Saved table for season 2020-2021 to data/abschlusstabellen\\2020-2021.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100159.html\n",
      "Saved table for season 2021-2022 to data/abschlusstabellen\\2021-2022.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100160.html\n",
      "Saved table for season 2022-2023 to data/abschlusstabellen\\2022-2023.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100161.html\n",
      "Saved table for season 2023-2024 to data/abschlusstabellen\\2023-2024.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Directory to store CSV files\n",
    "output_dir = \"data/first/abschlusstabellen\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over pages from 101 to 161 inclusive\n",
    "for i in range(101, 162):\n",
    "    url = f\"http://www.bulibox.de/abschlusstabellen/B100{i}.html\"\n",
    "    print(f\"Scraping: {url}\")\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error retrieving {url}\")\n",
    "        continue\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    table = soup.find(\"table\", class_=\"abschluss\")\n",
    "    if not table:\n",
    "        print(f\"No table found at {url}\")\n",
    "        continue\n",
    "\n",
    "    # Parse table header\n",
    "    header_row = table.find(\"tr\")\n",
    "    headers = [th.get_text(strip=True) for th in header_row.find_all(\"th\")]\n",
    "    \n",
    "    # Parse table rows\n",
    "    rows = []\n",
    "    for tr in table.find_all(\"tr\")[1:]:\n",
    "        cells = tr.find_all(\"td\")\n",
    "        row = []\n",
    "        for cell in cells:\n",
    "            # If the cell contains a link, get its text; otherwise, use cell text\n",
    "            a_tag = cell.find(\"a\")\n",
    "            cell_text = a_tag.get_text(strip=True) if a_tag else cell.get_text(strip=True)\n",
    "            row.append(cell_text)\n",
    "        rows.append(row)\n",
    "    \n",
    "    # Create DataFrame from the table data\n",
    "    df_table = pd.DataFrame(rows, columns=headers)\n",
    "    \n",
    "    # Extract season from the \"Statistik\" column of the first row, e.g., \"Saison 2023/2024\"\n",
    "    if \"Statistik\" in df_table.columns and not df_table.empty:\n",
    "        season_text = df_table.loc[0, \"Statistik\"]\n",
    "        season = season_text.replace(\"Saison\", \"\").strip()\n",
    "        # Replace slashes with dashes to avoid directory issues (e.g., \"2023/2024\" -> \"2023-2024\")\n",
    "        season = season.replace(\"/\", \"-\")\n",
    "    else:\n",
    "        season = f\"season_{i}\"\n",
    "    \n",
    "    # Build output path and save DataFrame as CSV\n",
    "    output_path = os.path.join(output_dir, f\"{season}.csv\")\n",
    "    df_table.to_csv(output_path, index=False)\n",
    "    print(f\"Saved table for season {season} to {output_path}\")\n",
    "    \n",
    "    # Wait 5 seconds before the next request\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gasther all results from second bl\n",
    "This is mandatory since each team can drop to the second BL. I will gather all results from the second BL as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: http://www.bulibox.de/spieltage/B200250.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200249.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200248.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200247.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200246.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200245.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200244.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200243.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200242.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200241.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200240.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200239.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200238.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200237.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200236.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200235.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200234.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200233.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200232.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200231.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200230.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200229.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200228.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200227.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200226.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200225.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200224.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200223.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200222.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200221.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200220.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200219.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B2N0218.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B2S0218.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200217.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200216.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200215.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200214.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200213.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200212.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200211.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200210.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200209.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B200208.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B2N0207.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B2S0207.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B2N0206.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B2S0206.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B2N0205.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B2S0205.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B2N0204.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B2S0204.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B2N0203.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B2S0203.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B2N0202.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B2S0202.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B2N0201.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B2S0201.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Season</th>\n",
       "      <th>Spieltag</th>\n",
       "      <th>Spielpaarung</th>\n",
       "      <th>Ergebnis</th>\n",
       "      <th>Datum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2. Bundesliga - Saison 2023/2024: Alle Spiele</td>\n",
       "      <td>1. Spieltag</td>\n",
       "      <td>Hamburger SV - FC Schalke 04</td>\n",
       "      <td>5:3</td>\n",
       "      <td>28.07.2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2. Bundesliga - Saison 2023/2024: Alle Spiele</td>\n",
       "      <td>1. Spieltag</td>\n",
       "      <td>1. FC Kaiserslautern - FC St. Pauli</td>\n",
       "      <td>1:2</td>\n",
       "      <td>29.07.2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2. Bundesliga - Saison 2023/2024: Alle Spiele</td>\n",
       "      <td>1. Spieltag</td>\n",
       "      <td>Hannover 96 - SV Elversberg</td>\n",
       "      <td>2:2</td>\n",
       "      <td>29.07.2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2. Bundesliga - Saison 2023/2024: Alle Spiele</td>\n",
       "      <td>1. Spieltag</td>\n",
       "      <td>VfL Osnabrück - Karlsruher SC</td>\n",
       "      <td>2:3</td>\n",
       "      <td>29.07.2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2. Bundesliga - Saison 2023/2024: Alle Spiele</td>\n",
       "      <td>1. Spieltag</td>\n",
       "      <td>SV Wehen Wiesbaden - 1. FC Magdeburg</td>\n",
       "      <td>1:1</td>\n",
       "      <td>29.07.2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19613</th>\n",
       "      <td>2. Bundesliga SÜD - Saison 1974/1975</td>\n",
       "      <td>38. Spieltag</td>\n",
       "      <td>Wormatia Worms - 1. FSV Mainz 05</td>\n",
       "      <td>2:3</td>\n",
       "      <td>15.06.1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19614</th>\n",
       "      <td>2. Bundesliga SÜD - Saison 1974/1975</td>\n",
       "      <td>38. Spieltag</td>\n",
       "      <td>1. FC Saarbrücken - Stuttgarter Kickers</td>\n",
       "      <td>1:1</td>\n",
       "      <td>15.06.1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19615</th>\n",
       "      <td>2. Bundesliga SÜD - Saison 1974/1975</td>\n",
       "      <td>38. Spieltag</td>\n",
       "      <td>FC Schweinfurt 05 - Chio Waldhof 07</td>\n",
       "      <td>2:0</td>\n",
       "      <td>15.06.1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19616</th>\n",
       "      <td>2. Bundesliga SÜD - Saison 1974/1975</td>\n",
       "      <td>38. Spieltag</td>\n",
       "      <td>FC Homburg - SpVgg Bayreuth</td>\n",
       "      <td>1:0</td>\n",
       "      <td>15.06.1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19617</th>\n",
       "      <td>2. Bundesliga SÜD - Saison 1974/1975</td>\n",
       "      <td>38. Spieltag</td>\n",
       "      <td>VfR Mannheim - SpVgg Fürth</td>\n",
       "      <td>2:2</td>\n",
       "      <td>15.06.1975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19618 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Season      Spieltag  \\\n",
       "0      2. Bundesliga - Saison 2023/2024: Alle Spiele   1. Spieltag   \n",
       "1      2. Bundesliga - Saison 2023/2024: Alle Spiele   1. Spieltag   \n",
       "2      2. Bundesliga - Saison 2023/2024: Alle Spiele   1. Spieltag   \n",
       "3      2. Bundesliga - Saison 2023/2024: Alle Spiele   1. Spieltag   \n",
       "4      2. Bundesliga - Saison 2023/2024: Alle Spiele   1. Spieltag   \n",
       "...                                              ...           ...   \n",
       "19613           2. Bundesliga SÜD - Saison 1974/1975  38. Spieltag   \n",
       "19614           2. Bundesliga SÜD - Saison 1974/1975  38. Spieltag   \n",
       "19615           2. Bundesliga SÜD - Saison 1974/1975  38. Spieltag   \n",
       "19616           2. Bundesliga SÜD - Saison 1974/1975  38. Spieltag   \n",
       "19617           2. Bundesliga SÜD - Saison 1974/1975  38. Spieltag   \n",
       "\n",
       "                                  Spielpaarung Ergebnis       Datum  \n",
       "0                 Hamburger SV - FC Schalke 04      5:3  28.07.2023  \n",
       "1          1. FC Kaiserslautern - FC St. Pauli      1:2  29.07.2023  \n",
       "2                  Hannover 96 - SV Elversberg      2:2  29.07.2023  \n",
       "3                VfL Osnabrück - Karlsruher SC      2:3  29.07.2023  \n",
       "4         SV Wehen Wiesbaden - 1. FC Magdeburg      1:1  29.07.2023  \n",
       "...                                        ...      ...         ...  \n",
       "19613         Wormatia Worms - 1. FSV Mainz 05      2:3  15.06.1975  \n",
       "19614  1. FC Saarbrücken - Stuttgarter Kickers      1:1  15.06.1975  \n",
       "19615      FC Schweinfurt 05 - Chio Waldhof 07      2:0  15.06.1975  \n",
       "19616              FC Homburg - SpVgg Bayreuth      1:0  15.06.1975  \n",
       "19617               VfR Mannheim - SpVgg Fürth      2:2  15.06.1975  \n",
       "\n",
       "[19618 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base URL of the main page\n",
    "base_url = \"http://www.bulibox.de/abschlusstabellen/index-2.liga.html\"\n",
    "\n",
    "# List to store scraped data\n",
    "data = []\n",
    "\n",
    "# Fetch the main page\n",
    "response = requests.get(base_url)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Error retrieving {base_url}\")\n",
    "    exit()\n",
    "\n",
    "main_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Extract all rows from the table\n",
    "rows = main_soup.find_all(\"tr\")\n",
    "\n",
    "# Iterate over each row to find season data\n",
    "for row in rows:\n",
    "    cells = row.find_all(\"td\")\n",
    "    if len(cells) >= 4:\n",
    "        # Extract the season description\n",
    "        season_desc = cells[0].get_text(strip=True)\n",
    "        \n",
    "        # Extract the 'Ergebnisse' link\n",
    "        ergebnisse_link = cells[2].find(\"a\", href=True)\n",
    "        if ergebnisse_link:\n",
    "            relative_url = ergebnisse_link['href']\n",
    "            # Construct the full URL for the 'Ergebnisse' page\n",
    "            season_url = \"http://www.bulibox.de\" + relative_url[2:]  # Adjusting the relative URL\n",
    "            print(f\"Scraping: {season_url}\")\n",
    "            \n",
    "            season_response = requests.get(season_url)\n",
    "            if season_response.status_code != 200:\n",
    "                print(f\"Error retrieving {season_url}\")\n",
    "                continue\n",
    "\n",
    "            season_soup = BeautifulSoup(season_response.content, \"html.parser\")\n",
    "\n",
    "            # Get season info from the <h4> tag (if available)\n",
    "            h4_tag = season_soup.find(\"h4\")\n",
    "            season = h4_tag.text.strip() if h4_tag else season_desc\n",
    "\n",
    "            # Find all matchday markers (<b class=\"bulired\">)\n",
    "            matchday_tags = season_soup.find_all(\"b\", class_=\"bulired\")\n",
    "\n",
    "            for matchday_tag in matchday_tags:\n",
    "                matchday = matchday_tag.text.strip()\n",
    "                # Get the table that immediately follows this matchday header\n",
    "                table = matchday_tag.find_next(\"table\")\n",
    "                if table:\n",
    "                    rows = table.find_all(\"tr\")[1:]  # Skip header row\n",
    "                    for row in rows:\n",
    "                        cells = row.find_all(\"td\")\n",
    "                        if len(cells) >= 3:\n",
    "                            spielpaarung = cells[0].get_text(strip=True)\n",
    "                            ergebnis = cells[1].get_text(strip=True)\n",
    "                            datum = cells[2].get_text(strip=True)\n",
    "                            \n",
    "                            data.append({\n",
    "                                \"Season\": season,\n",
    "                                \"Spieltag\": matchday,\n",
    "                                \"Spielpaarung\": spielpaarung,\n",
    "                                \"Ergebnis\": ergebnis,\n",
    "                                \"Datum\": datum\n",
    "                            })\n",
    "\n",
    "            # Wait 5 seconds before the next request to be compliant with the site's usage policy\n",
    "            time.sleep(5)\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV for later analysis\n",
    "df.to_csv(\"data/second/second_bl_results.csv\", index=False)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather second liga abschlusstabellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200250.html\n",
      "Saved table for season 2023-2024 to data/second/abschlusstabellen\\2023-2024.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200249.html\n",
      "Saved table for season 2022-2023 to data/second/abschlusstabellen\\2022-2023.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200248.html\n",
      "Saved table for season 2021-2022 to data/second/abschlusstabellen\\2021-2022.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200247.html\n",
      "Saved table for season 2020-2021 to data/second/abschlusstabellen\\2020-2021.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200246.html\n",
      "Saved table for season 2019-2020 to data/second/abschlusstabellen\\2019-2020.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200245.html\n",
      "Saved table for season 2018-2019 to data/second/abschlusstabellen\\2018-2019.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200244.html\n",
      "Saved table for season 2017-2018 to data/second/abschlusstabellen\\2017-2018.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200243.html\n",
      "Saved table for season 2016-2017 to data/second/abschlusstabellen\\2016-2017.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200242.html\n",
      "Saved table for season 2015-2016 to data/second/abschlusstabellen\\2015-2016.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200241.html\n",
      "Saved table for season 2014-2015 to data/second/abschlusstabellen\\2014-2015.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200240.html\n",
      "Saved table for season 2013-2014 to data/second/abschlusstabellen\\2013-2014.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200239.html\n",
      "Saved table for season 2012-2013 to data/second/abschlusstabellen\\2012-2013.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200238.html\n",
      "Saved table for season 2011-2012 to data/second/abschlusstabellen\\2011-2012.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200237.html\n",
      "Saved table for season 2010-2011 to data/second/abschlusstabellen\\2010-2011.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200236.html\n",
      "Saved table for season 2009-2010 to data/second/abschlusstabellen\\2009-2010.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200235.html\n",
      "Saved table for season 2008-2009 to data/second/abschlusstabellen\\2008-2009.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200234.html\n",
      "Saved table for season 2007-2008 to data/second/abschlusstabellen\\2007-2008.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200233.html\n",
      "Saved table for season 2006-2007 to data/second/abschlusstabellen\\2006-2007.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200232.html\n",
      "Saved table for season 2005-2006 to data/second/abschlusstabellen\\2005-2006.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200231.html\n",
      "Saved table for season 2004-2005 to data/second/abschlusstabellen\\2004-2005.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200230.html\n",
      "Saved table for season 2003-2004 to data/second/abschlusstabellen\\2003-2004.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200229.html\n",
      "Saved table for season 2002-2003 to data/second/abschlusstabellen\\2002-2003.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200228.html\n",
      "Saved table for season 2001-2002 to data/second/abschlusstabellen\\2001-2002.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200227.html\n",
      "Saved table for season 2000-2001 to data/second/abschlusstabellen\\2000-2001.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200226.html\n",
      "Saved table for season 1999-2000 to data/second/abschlusstabellen\\1999-2000.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200225.html\n",
      "Saved table for season 1998-1999 to data/second/abschlusstabellen\\1998-1999.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200224.html\n",
      "Saved table for season 1997-1998 to data/second/abschlusstabellen\\1997-1998.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200223.html\n",
      "Saved table for season 1996-1997 to data/second/abschlusstabellen\\1996-1997.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200222.html\n",
      "Saved table for season 1995-1996 to data/second/abschlusstabellen\\1995-1996.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200221.html\n",
      "Saved table for season 1994-1995 to data/second/abschlusstabellen\\1994-1995.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200220.html\n",
      "Saved table for season 1993-1994 to data/second/abschlusstabellen\\1993-1994.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200219.html\n",
      "Saved table for season 1992-1993 to data/second/abschlusstabellen\\1992-1993.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B2N0218.html\n",
      "Saved table for season 1991-1992 to data/second/abschlusstabellen\\1991-1992.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B2S0218.html\n",
      "Saved table for season 1991-1992 to data/second/abschlusstabellen\\1991-1992.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200217.html\n",
      "Saved table for season 1990-1991 to data/second/abschlusstabellen\\1990-1991.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200216.html\n",
      "Saved table for season 1989-1990 to data/second/abschlusstabellen\\1989-1990.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200215.html\n",
      "Saved table for season 1988-1989 to data/second/abschlusstabellen\\1988-1989.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200214.html\n",
      "Saved table for season 1987-1988 to data/second/abschlusstabellen\\1987-1988.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200213.html\n",
      "Saved table for season 1986-1987 to data/second/abschlusstabellen\\1986-1987.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200212.html\n",
      "Saved table for season 1985-1986 to data/second/abschlusstabellen\\1985-1986.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200211.html\n",
      "Saved table for season 1984-1985 to data/second/abschlusstabellen\\1984-1985.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200210.html\n",
      "Saved table for season 1983-1984 to data/second/abschlusstabellen\\1983-1984.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200209.html\n",
      "Saved table for season 1982-1983 to data/second/abschlusstabellen\\1982-1983.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B200208.html\n",
      "Saved table for season 1981-1982 to data/second/abschlusstabellen\\1981-1982.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B2N0207.html\n",
      "Saved table for season 1980-1981 to data/second/abschlusstabellen\\1980-1981.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B2S0207.html\n",
      "Saved table for season 1980-1981 to data/second/abschlusstabellen\\1980-1981.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B2N0206.html\n",
      "Saved table for season 1979-1980 to data/second/abschlusstabellen\\1979-1980.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B2S0206.html\n",
      "Saved table for season 1979-1980 to data/second/abschlusstabellen\\1979-1980.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B2N0205.html\n",
      "Saved table for season 1978-1979 to data/second/abschlusstabellen\\1978-1979.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B2S0205.html\n",
      "Saved table for season 1978-1979 to data/second/abschlusstabellen\\1978-1979.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B2N0204.html\n",
      "Saved table for season 1977-1978 to data/second/abschlusstabellen\\1977-1978.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B2S0204.html\n",
      "Saved table for season 1977-1978 to data/second/abschlusstabellen\\1977-1978.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B2N0203.html\n",
      "Saved table for season 1976-1977 to data/second/abschlusstabellen\\1976-1977.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B2S0203.html\n",
      "Saved table for season 1976-1977 to data/second/abschlusstabellen\\1976-1977.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B2N0202.html\n",
      "Saved table for season 1975-1976 to data/second/abschlusstabellen\\1975-1976.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B2S0202.html\n",
      "Saved table for season 1975-1976 to data/second/abschlusstabellen\\1975-1976.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B2N0201.html\n",
      "Saved table for season 1974-1975 to data/second/abschlusstabellen\\1974-1975.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B2S0201.html\n",
      "Saved table for season 1974-1975 to data/second/abschlusstabellen\\1974-1975.csv\n"
     ]
    }
   ],
   "source": [
    "#Base URL of the main page\n",
    "base_url = \"http://www.bulibox.de/abschlusstabellen/index-2.liga.html\"\n",
    "\n",
    "# Directory to store CSV files\n",
    "output_dir = \"data/second/abschlusstabellen\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Fetch the main page\n",
    "response = requests.get(base_url)\n",
    "if response.status_code != 200:\n",
    "    print(f\"Error retrieving {base_url}\")\n",
    "    exit()\n",
    "\n",
    "main_soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Extract all rows from the table\n",
    "rows = main_soup.find_all(\"tr\")\n",
    "\n",
    "# Iterate over each row to find season data\n",
    "for row in rows:\n",
    "    cells = row.find_all(\"td\")\n",
    "    if len(cells) >= 4:\n",
    "        # Extract the 'Abschlusstabelle' link\n",
    "        tabelle_link = cells[1].find(\"a\", href=True)\n",
    "        if tabelle_link:\n",
    "            relative_url = tabelle_link['href']\n",
    "            # Construct the full URL for the 'Abschlusstabelle' page\n",
    "            season_url = \"http://www.bulibox.de/abschlusstabellen/\" + relative_url\n",
    "            print(f\"Scraping: {season_url}\")\n",
    "            \n",
    "            season_response = requests.get(season_url)\n",
    "            if season_response.status_code != 200:\n",
    "                print(f\"Error retrieving {season_url}\")\n",
    "                continue\n",
    "\n",
    "            season_soup = BeautifulSoup(season_response.content, \"html.parser\")\n",
    "            table = season_soup.find(\"table\", class_=\"abschluss\")\n",
    "            if not table:\n",
    "                print(f\"No table found at {season_url}\")\n",
    "                continue\n",
    "\n",
    "            # Parse table header\n",
    "            header_row = table.find(\"tr\")\n",
    "            headers = [th.get_text(strip=True) for th in header_row.find_all(\"th\")]\n",
    "            \n",
    "            # Parse table rows\n",
    "            rows = []\n",
    "            for tr in table.find_all(\"tr\")[1:]:\n",
    "                cells = tr.find_all(\"td\")\n",
    "                row = [cell.get_text(strip=True) for cell in cells]\n",
    "                rows.append(row)\n",
    "            \n",
    "            # Create DataFrame from the table data\n",
    "            df_table = pd.DataFrame(rows, columns=headers)\n",
    "            \n",
    "            # Extract season from the link's title attribute, e.g., \"Abschlusstabelle 2023/2024\"\n",
    "            season_text = tabelle_link.get('title', 'Unknown Season')\n",
    "            season = season_text.replace(\"Abschlusstabelle\", \"\").strip()\n",
    "            # Replace slashes with dashes to avoid directory issues (e.g., \"2023/2024\" -> \"2023-2024\")\n",
    "            season = season.replace(\"/\", \"-\")\n",
    "            \n",
    "            # Build output path and save DataFrame as CSV\n",
    "            output_path = os.path.join(output_dir, f\"{season}.csv\")\n",
    "            df_table.to_csv(output_path, index=False)\n",
    "            print(f\"Saved table for season {season} to {output_path}\")\n",
    "            \n",
    "            # Wait 5 seconds before the next request\n",
    "            time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
