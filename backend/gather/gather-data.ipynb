{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Historic Data\n",
    "This notebook is, for now, more of experimental work. I want to gather multiple sources of data - preferably automate this process. For starters I want to gather historical data since they are more of a one time job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Start with all results from the first BL\n",
    "\n",
    "I found multiple websites to gather. I took this one that is the simplest to scrap. Since no `robot.txt` is present I dont think this is a problem, because I only want to gather this one time only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: http://www.bulibox.de/spieltage/B100101.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100102.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100103.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100104.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100105.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100106.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100107.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100108.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100109.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100110.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100111.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100112.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100113.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100114.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100115.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100116.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100117.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100118.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100119.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100120.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100121.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100122.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100123.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100124.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100125.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100126.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100127.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100128.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100129.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100130.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100131.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100132.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100133.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100134.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100135.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100136.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100137.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100138.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100139.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100140.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100141.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100142.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100143.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100144.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100145.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100146.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100147.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100148.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100149.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100150.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100151.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100152.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100153.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100154.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100155.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100156.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100157.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100158.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100159.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100160.html\n",
      "Scraping: http://www.bulibox.de/spieltage/B100161.html\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Columns must be same length as key",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Split the 'Spielpaarung' column into 'HomeTeam' and 'GuestTeam'\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHomeTeam\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGuestTeam\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSpielpaarung\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, expand\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     58\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHomeTeam\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHomeTeam\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     59\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGuestTeam\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGuestTeam\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\Users\\MBUL\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4299\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_frame(key, value)\n\u001b[0;32m   4298\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, (Series, np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mlist\u001b[39m, Index)):\n\u001b[1;32m-> 4299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array(key, value)\n\u001b[0;32m   4300\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, DataFrame):\n\u001b[0;32m   4301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_item_frame_value(key, value)\n",
      "File \u001b[1;32mc:\\Users\\MBUL\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4341\u001b[0m, in \u001b[0;36mDataFrame._setitem_array\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4336\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4337\u001b[0m     \u001b[38;5;66;03m# Note: unlike self.iloc[:, indexer] = value, this will\u001b[39;00m\n\u001b[0;32m   4338\u001b[0m     \u001b[38;5;66;03m#  never try to overwrite values inplace\u001b[39;00m\n\u001b[0;32m   4340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, DataFrame):\n\u001b[1;32m-> 4341\u001b[0m         check_key_length(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, key, value)\n\u001b[0;32m   4342\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k1, k2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(key, value\u001b[38;5;241m.\u001b[39mcolumns):\n\u001b[0;32m   4343\u001b[0m             \u001b[38;5;28mself\u001b[39m[k1] \u001b[38;5;241m=\u001b[39m value[k2]\n",
      "File \u001b[1;32mc:\\Users\\MBUL\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexers\\utils.py:390\u001b[0m, in \u001b[0;36mcheck_key_length\u001b[1;34m(columns, key, value)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m columns\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(key):\n\u001b[1;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns must be same length as key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;66;03m# Missing keys in columns are represented as -1\u001b[39;00m\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(key)[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(value\u001b[38;5;241m.\u001b[39mcolumns):\n",
      "\u001b[1;31mValueError\u001b[0m: Columns must be same length as key"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# List to store scraped data\n",
    "data = []\n",
    "\n",
    "# Iterate over the pages (from 101 to 161 inclusive)\n",
    "for i in range(101, 162):\n",
    "    url = f\"http://www.bulibox.de/spieltage/B100{i}.html\"\n",
    "    print(f\"Scraping: {url}\")\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the page was retrieved successfully\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error retrieving {url}\")\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Get season info from the <h4> tag (if available)\n",
    "    h4_tag = soup.find(\"h4\")\n",
    "    season = h4_tag.text.strip() if h4_tag else \"Unknown Season\"\n",
    "    \n",
    "    # Find all matchday markers (<b class=\"bulired\">)\n",
    "    matchday_tags = soup.find_all(\"b\", class_=\"bulired\")\n",
    "    \n",
    "    for matchday_tag in matchday_tags:\n",
    "        matchday = matchday_tag.text.strip()\n",
    "        # Get the table that immediately follows this matchday header\n",
    "        table = matchday_tag.find_next(\"table\")\n",
    "        if table:\n",
    "            rows = table.find_all(\"tr\")[1:]  # skip header row\n",
    "            for row in rows:\n",
    "                cells = row.find_all(\"td\")\n",
    "                if len(cells) >= 3:\n",
    "                    spielpaarung = cells[0].get_text(strip=True)\n",
    "                    ergebnis = cells[1].get_text(strip=True)\n",
    "                    datum = cells[2].get_text(strip=True)\n",
    "                    \n",
    "                    data.append({\n",
    "                        \"Season\": season,\n",
    "                        \"Spieltag\": matchday,\n",
    "                        \"Spielpaarung\": spielpaarung,\n",
    "                        \"Ergebnis\": ergebnis,\n",
    "                        \"Datum\": datum\n",
    "                    })\n",
    "    \n",
    "    # Wait 5 seconds before the next request to be compliant with the site's usage\n",
    "    time.sleep(5)\n",
    "\n",
    "# Create a DataFrame from the collected data\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"first_bl_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather all tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100101.html\n",
      "Saved table for season 1963-1964 to data/abschlusstabellen\\1963-1964.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100102.html\n",
      "Saved table for season 1964-1965 to data/abschlusstabellen\\1964-1965.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100103.html\n",
      "Saved table for season 1965-1966 to data/abschlusstabellen\\1965-1966.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100104.html\n",
      "Saved table for season 1966-1967 to data/abschlusstabellen\\1966-1967.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100105.html\n",
      "Saved table for season 1967-1968 to data/abschlusstabellen\\1967-1968.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100106.html\n",
      "Saved table for season 1968-1969 to data/abschlusstabellen\\1968-1969.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100107.html\n",
      "Saved table for season 1969-1970 to data/abschlusstabellen\\1969-1970.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100108.html\n",
      "Saved table for season 1970-1971 to data/abschlusstabellen\\1970-1971.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100109.html\n",
      "Saved table for season 1971-1972 to data/abschlusstabellen\\1971-1972.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100110.html\n",
      "Saved table for season 1972-1973 to data/abschlusstabellen\\1972-1973.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100111.html\n",
      "Saved table for season 1973-1974 to data/abschlusstabellen\\1973-1974.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100112.html\n",
      "Saved table for season 1974-1975 to data/abschlusstabellen\\1974-1975.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100113.html\n",
      "Saved table for season 1975-1976 to data/abschlusstabellen\\1975-1976.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100114.html\n",
      "Saved table for season 1976-1977 to data/abschlusstabellen\\1976-1977.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100115.html\n",
      "Saved table for season 1977-1978 to data/abschlusstabellen\\1977-1978.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100116.html\n",
      "Saved table for season 1978-1979 to data/abschlusstabellen\\1978-1979.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100117.html\n",
      "Saved table for season 1979-1980 to data/abschlusstabellen\\1979-1980.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100118.html\n",
      "Saved table for season 1980-1981 to data/abschlusstabellen\\1980-1981.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100119.html\n",
      "Saved table for season 1981-1982 to data/abschlusstabellen\\1981-1982.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100120.html\n",
      "Saved table for season 1982-1983 to data/abschlusstabellen\\1982-1983.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100121.html\n",
      "Saved table for season 1983-1984 to data/abschlusstabellen\\1983-1984.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100122.html\n",
      "Saved table for season 1984-1985 to data/abschlusstabellen\\1984-1985.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100123.html\n",
      "Saved table for season 1985-1986 to data/abschlusstabellen\\1985-1986.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100124.html\n",
      "Saved table for season 1986-1987 to data/abschlusstabellen\\1986-1987.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100125.html\n",
      "Saved table for season 1987-1988 to data/abschlusstabellen\\1987-1988.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100126.html\n",
      "Saved table for season 1988-1989 to data/abschlusstabellen\\1988-1989.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100127.html\n",
      "Saved table for season 1989-1990 to data/abschlusstabellen\\1989-1990.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100128.html\n",
      "Saved table for season 1990-1991 to data/abschlusstabellen\\1990-1991.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100129.html\n",
      "Saved table for season 1991-1992 to data/abschlusstabellen\\1991-1992.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100130.html\n",
      "Saved table for season 1992-1993 to data/abschlusstabellen\\1992-1993.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100131.html\n",
      "Saved table for season 1993-1994 to data/abschlusstabellen\\1993-1994.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100132.html\n",
      "Saved table for season 1994-1995 to data/abschlusstabellen\\1994-1995.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100133.html\n",
      "Saved table for season 1995-1996 to data/abschlusstabellen\\1995-1996.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100134.html\n",
      "Saved table for season 1996-1997 to data/abschlusstabellen\\1996-1997.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100135.html\n",
      "Saved table for season 1997-1998 to data/abschlusstabellen\\1997-1998.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100136.html\n",
      "Saved table for season 1998-1999 to data/abschlusstabellen\\1998-1999.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100137.html\n",
      "Saved table for season 1999-2000 to data/abschlusstabellen\\1999-2000.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100138.html\n",
      "Saved table for season 2000-2001 to data/abschlusstabellen\\2000-2001.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100139.html\n",
      "Saved table for season 2001-2002 to data/abschlusstabellen\\2001-2002.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100140.html\n",
      "Saved table for season 2002-2003 to data/abschlusstabellen\\2002-2003.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100141.html\n",
      "Saved table for season 2003-2004 to data/abschlusstabellen\\2003-2004.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100142.html\n",
      "Saved table for season 2004-2005 to data/abschlusstabellen\\2004-2005.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100143.html\n",
      "Saved table for season 2005-2006 to data/abschlusstabellen\\2005-2006.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100144.html\n",
      "Saved table for season 2006-2007 to data/abschlusstabellen\\2006-2007.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100145.html\n",
      "Saved table for season 2007-2008 to data/abschlusstabellen\\2007-2008.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100146.html\n",
      "Saved table for season 2008-2009 to data/abschlusstabellen\\2008-2009.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100147.html\n",
      "Saved table for season 2009-2010 to data/abschlusstabellen\\2009-2010.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100148.html\n",
      "Saved table for season 2010-2011 to data/abschlusstabellen\\2010-2011.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100149.html\n",
      "Saved table for season 2011-2012 to data/abschlusstabellen\\2011-2012.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100150.html\n",
      "Saved table for season 2012-2013 to data/abschlusstabellen\\2012-2013.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100151.html\n",
      "Saved table for season 2013-2014 to data/abschlusstabellen\\2013-2014.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100152.html\n",
      "Saved table for season 2014-2015 to data/abschlusstabellen\\2014-2015.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100153.html\n",
      "Saved table for season 2015-2016 to data/abschlusstabellen\\2015-2016.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100154.html\n",
      "Saved table for season 2016-2017 to data/abschlusstabellen\\2016-2017.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100155.html\n",
      "Saved table for season 2017-2018 to data/abschlusstabellen\\2017-2018.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100156.html\n",
      "Saved table for season 2018-2019 to data/abschlusstabellen\\2018-2019.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100157.html\n",
      "Saved table for season 2019-2020 to data/abschlusstabellen\\2019-2020.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100158.html\n",
      "Saved table for season 2020-2021 to data/abschlusstabellen\\2020-2021.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100159.html\n",
      "Saved table for season 2021-2022 to data/abschlusstabellen\\2021-2022.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100160.html\n",
      "Saved table for season 2022-2023 to data/abschlusstabellen\\2022-2023.csv\n",
      "Scraping: http://www.bulibox.de/abschlusstabellen/B100161.html\n",
      "Saved table for season 2023-2024 to data/abschlusstabellen\\2023-2024.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Directory to store CSV files\n",
    "output_dir = \"data/abschlusstabellen\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over pages from 101 to 161 inclusive\n",
    "for i in range(101, 162):\n",
    "    url = f\"http://www.bulibox.de/abschlusstabellen/B100{i}.html\"\n",
    "    print(f\"Scraping: {url}\")\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error retrieving {url}\")\n",
    "        continue\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    table = soup.find(\"table\", class_=\"abschluss\")\n",
    "    if not table:\n",
    "        print(f\"No table found at {url}\")\n",
    "        continue\n",
    "\n",
    "    # Parse table header\n",
    "    header_row = table.find(\"tr\")\n",
    "    headers = [th.get_text(strip=True) for th in header_row.find_all(\"th\")]\n",
    "    \n",
    "    # Parse table rows\n",
    "    rows = []\n",
    "    for tr in table.find_all(\"tr\")[1:]:\n",
    "        cells = tr.find_all(\"td\")\n",
    "        row = []\n",
    "        for cell in cells:\n",
    "            # If the cell contains a link, get its text; otherwise, use cell text\n",
    "            a_tag = cell.find(\"a\")\n",
    "            cell_text = a_tag.get_text(strip=True) if a_tag else cell.get_text(strip=True)\n",
    "            row.append(cell_text)\n",
    "        rows.append(row)\n",
    "    \n",
    "    # Create DataFrame from the table data\n",
    "    df_table = pd.DataFrame(rows, columns=headers)\n",
    "    \n",
    "    # Extract season from the \"Statistik\" column of the first row, e.g., \"Saison 2023/2024\"\n",
    "    if \"Statistik\" in df_table.columns and not df_table.empty:\n",
    "        season_text = df_table.loc[0, \"Statistik\"]\n",
    "        season = season_text.replace(\"Saison\", \"\").strip()\n",
    "        # Replace slashes with dashes to avoid directory issues (e.g., \"2023/2024\" -> \"2023-2024\")\n",
    "        season = season.replace(\"/\", \"-\")\n",
    "    else:\n",
    "        season = f\"season_{i}\"\n",
    "    \n",
    "    # Build output path and save DataFrame as CSV\n",
    "    output_path = os.path.join(output_dir, f\"{season}.csv\")\n",
    "    df_table.to_csv(output_path, index=False)\n",
    "    print(f\"Saved table for season {season} to {output_path}\")\n",
    "    \n",
    "    # Wait 5 seconds before the next request\n",
    "    time.sleep(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
